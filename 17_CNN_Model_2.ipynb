{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5bcb72-be00-4eed-a5ab-8645713a6792",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3965f-3be6-4c8e-bf4d-ae9ba0adfa89",
   "metadata": {},
   "source": [
    "# 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1cc10d7-e331-4eb5-8a33-8ca765da7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 20\n",
    "    scheduler_gamma = 0.9\n",
    "    train_loader = None\n",
    "    test_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27704c8a-6c84-4362-8949-00720cb5ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61364824-0dfe-47de-b8c5-f148e917dd4b",
   "metadata": {},
   "source": [
    "## 0.1. Load Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28feef2a-01f0-4283-a1a5-2fbc3ad96bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ec5df-a61a-4386-970a-95be70167d58",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7f24-dc79-4944-a623-da8e92b3ad91",
   "metadata": {},
   "source": [
    "## 1.1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b137adf-d407-49fb-bc17-789fa27f71e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebde338a-3ff0-4420-8c56-1811d06872fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d76bc2abbb4157bcfd6bb1ecc2a589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "prepared_dataset = load_dataset(\"./prepared_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3a58dd-8f3a-49f9-9e96-9a957c299f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels', 'class'],\n",
       "        num_rows: 66514\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제대로 불러왔는지 테스트해보자.\n",
    "prepared_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0187a38-5b13-4983-ba36-b974268257a9",
   "metadata": {},
   "source": [
    "## 1.2. Split Train Dataset and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14859d0-44a8-49a0-8910-249d6c68d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset과 test dataset으로 분리한다.\n",
    "split_dataset = prepared_dataset['train'].train_test_split(test_size=0.01, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595a13da-d662-49e5-8d60-44fdfe7f2cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels', 'class'],\n",
       "        num_rows: 65848\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels', 'class'],\n",
       "        num_rows: 666\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6eac7af-6b67-4059-a20a-1c754b4f3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7bef724-5dd7-47d4-af53-828b3b0ec17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[ 0.3997,  0.5925,  0.5563,  ...,  0.4520,  0.4180,  0.5115],\n",
       "         [ 0.7106,  0.7089,  0.4134,  ...,  0.4954,  0.3103,  0.6480],\n",
       "         [ 0.6471,  0.6571,  0.6065,  ...,  0.4870,  0.4367,  0.5973],\n",
       "         ...,\n",
       "         [-0.3370, -0.3724, -0.3525,  ..., -0.2762, -0.3248, -0.2963],\n",
       "         [-0.3280, -0.3598, -0.4122,  ..., -0.3429, -0.3176, -0.2677],\n",
       "         [-0.2644, -0.5422, -0.5633,  ..., -0.3347, -0.3671, -0.3838]]),\n",
       " 'labels': tensor([50258, 50264, 50359, 50363, 50257]),\n",
       " 'class': tensor(16)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce703e20-3562-4c38-baf5-c245a0f84120",
   "metadata": {},
   "source": [
    "## 1.3. Data Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9993bcf-ec6d-41d6-aec9-d45f6395a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 인풋 데이터와 라벨 데이터의 길이가 다르며, 따라서 서로 다른 패딩 방법이 적용되어야 한다. 그러므로 두 데이터를 분리해야 한다.\n",
    "        # 먼저 오디오 인풋 데이터를 간단히 토치 텐서로 반환하는 작업을 수행한다.\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Tokenize된 레이블 시퀀스를 가져온다.\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # 레이블 시퀀스에 대해 최대 길이만큼 패딩 작업을 실시한다.\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 패딩 토큰을 -100으로 치환하여 loss 계산 과정에서 무시되도록 한다.\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라낸다.\n",
    "        # 해당 토큰은 이후 언제든 추가할 수 있다.\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        class_list = [feature[\"class\"] for feature in features]\n",
    "        classes = torch.stack(class_list)\n",
    "        classes -= 1 # torch.tensor([1, 2, 3, ..., 19])를 torch.tensor([0, 1, 2, ..., 18])로 변경\n",
    "\n",
    "        return batch, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59afce99-80b1-42ed-ba64-269484b06c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713da01-2977-42f4-8262-73189758398a",
   "metadata": {},
   "source": [
    "## 1.4. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1490026-408b-4efd-979c-3d756ed5100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=Config.batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ca78b-e36c-4333-ae8a-ec9d4ed2a75f",
   "metadata": {},
   "source": [
    "- config에 train_loader와 test_loader를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecaaa24b-5520-43e2-aa40-afefcc0bbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_loader = train_dataloader\n",
    "config.test_loader = test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d7b4f1c-b217-4b3d-9ab4-373bfc126a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_features': tensor([[[ 0.3997,  0.5925,  0.5563,  ...,  0.4520,  0.4180,  0.5115],\n",
       "          [ 0.7106,  0.7089,  0.4134,  ...,  0.4954,  0.3103,  0.6480],\n",
       "          [ 0.6471,  0.6571,  0.6065,  ...,  0.4870,  0.4367,  0.5973],\n",
       "          ...,\n",
       "          [-0.3370, -0.3724, -0.3525,  ..., -0.2762, -0.3248, -0.2963],\n",
       "          [-0.3280, -0.3598, -0.4122,  ..., -0.3429, -0.3176, -0.2677],\n",
       "          [-0.2644, -0.5422, -0.5633,  ..., -0.3347, -0.3671, -0.3838]],\n",
       " \n",
       "         [[ 0.0030, -0.3602, -0.4643,  ..., -0.4643, -0.4643, -0.4643],\n",
       "          [ 0.0299, -0.1788, -0.2602,  ..., -0.4643, -0.4643, -0.4643],\n",
       "          [ 0.1383, -0.0552, -0.3880,  ..., -0.4643, -0.4643, -0.4643],\n",
       "          ...,\n",
       "          [-0.4643, -0.4643, -0.4643,  ..., -0.4643, -0.4643, -0.4643],\n",
       "          [-0.4643, -0.4643, -0.4643,  ..., -0.4643, -0.4643, -0.4643],\n",
       "          [-0.4643, -0.4643, -0.4643,  ..., -0.4643, -0.4643, -0.4643]],\n",
       " \n",
       "         [[-0.4043,  0.1011,  0.2221,  ..., -0.6095, -0.6095, -0.6095],\n",
       "          [-0.6095,  0.0027,  0.1194,  ..., -0.6095, -0.6095, -0.6095],\n",
       "          [-0.2477, -0.1811, -0.0933,  ..., -0.6095, -0.6095, -0.6095],\n",
       "          ...,\n",
       "          [-0.6095, -0.6095, -0.6095,  ..., -0.6095, -0.6095, -0.6095],\n",
       "          [-0.6095, -0.6095, -0.6095,  ..., -0.6095, -0.6095, -0.6095],\n",
       "          [-0.6095, -0.6095, -0.6095,  ..., -0.6095, -0.6095, -0.6095]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4214, -0.5073, -0.3023,  ..., -0.7019, -0.7019, -0.7019],\n",
       "          [-0.6025, -0.6632, -0.2273,  ..., -0.7019, -0.7019, -0.7019],\n",
       "          [-0.2991, -0.4518, -0.1938,  ..., -0.7019, -0.7019, -0.7019],\n",
       "          ...,\n",
       "          [-0.7019, -0.7019, -0.7019,  ..., -0.7019, -0.7019, -0.7019],\n",
       "          [-0.7019, -0.7019, -0.7019,  ..., -0.7019, -0.7019, -0.7019],\n",
       "          [-0.7019, -0.7019, -0.7019,  ..., -0.7019, -0.7019, -0.7019]],\n",
       " \n",
       "         [[-0.5196, -0.3677, -0.3225,  ..., -0.5196, -0.5196, -0.5196],\n",
       "          [-0.5196, -0.4803, -0.3958,  ..., -0.5196, -0.5196, -0.5196],\n",
       "          [-0.5189, -0.4373, -0.5132,  ..., -0.5196, -0.5196, -0.5196],\n",
       "          ...,\n",
       "          [-0.5196, -0.5196, -0.5196,  ..., -0.5196, -0.5196, -0.5196],\n",
       "          [-0.5196, -0.5196, -0.5196,  ..., -0.5196, -0.5196, -0.5196],\n",
       "          [-0.5196, -0.5196, -0.5196,  ..., -0.5196, -0.5196, -0.5196]],\n",
       " \n",
       "         [[ 0.1614, -0.2878, -0.4465,  ..., -0.7420, -0.7420, -0.7420],\n",
       "          [ 0.1925, -0.1756, -0.1566,  ..., -0.7420, -0.7420, -0.7420],\n",
       "          [ 0.1133, -0.0076, -0.0803,  ..., -0.7420, -0.7420, -0.7420],\n",
       "          ...,\n",
       "          [-0.5979, -0.5284, -0.5797,  ..., -0.7420, -0.7420, -0.7420],\n",
       "          [-0.5472, -0.5676, -0.6301,  ..., -0.7420, -0.7420, -0.7420],\n",
       "          [-0.7248, -0.7420, -0.7151,  ..., -0.7420, -0.7420, -0.7420]]]), 'labels': tensor([[50258, 50264, 50359,  ...,  -100,  -100,  -100],\n",
       "         [50258, 50264, 50359,  ...,  -100,  -100,  -100],\n",
       "         [50258, 50264, 50359,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [50258, 50264, 50359,  ...,  -100,  -100,  -100],\n",
       "         [50258, 50264, 50359,  ...,  -100,  -100,  -100],\n",
       "         [50258, 50264, 50359,  ...,  -100,  -100,  -100]])},\n",
       " tensor([15,  1,  5,  8,  7, 18, 12,  8, 16, 17, 18,  7,  0,  4, 12,  3,  6,  7,\n",
       "         14,  9,  3,  1, 13,  6, 18, 11, 13,  7, 10,  6, 17,  7,  2, 16,  0, 17,\n",
       "          4,  4,  1,  0, 18,  8, 17, 10, 18,  2, 13, 16, 18, 16,  4, 13,  5, 16,\n",
       "         18,  1, 18,  2, 18,  7, 17,  1,  7,  3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee1e72-c581-47ae-9ead-796041f124fd",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81b0614-91f1-401d-b908-d34e447f2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=(1, 2), padding=(1, 1)), # (?, 1, 80, 3000) -> (?, 8, 80, 1500)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # (?, 8, 80, 1500) -> (?, 8, 40, 750)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=(1, 2), padding=(1, 2)), # (?, 8, 40, 750) -> (?, 16, 40, 376)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # (?, 16, 40, 376) -> (?, 16, 20, 188)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=(1, 2), padding=(1, 1)), # (?, 16, 20, 188) -> (?, 32, 20, 94)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # (?, 32, 20, 94) -> (?, 32, 10, 47)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=(1, 2), padding=(1, 2)), # (?, 32, 10, 47) -> (?, 64, 10, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # (?, 64, 10, 24) -> (?, 64, 5, 12)\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=(1, 2), padding=(1, 1)), # (?, 64, 5, 12) -> (?, 128, 5, 6)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))) # (?, 128, 5, 6) -> (?, 128, 5, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(128*5*3, 128, bias=True) # (?, 256, 5, 3) -> (?, 128)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu') # He Initialization\n",
    "        self.linear1 = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2)) # 20% dropout\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64, bias=True) # (?, 128) -> (?, 64)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        self.linear2 = nn.Sequential(\n",
    "            self.fc2,\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2)) # 20% dropout\n",
    "\n",
    "        self.fc = nn.Linear(64, 19, bias=True) # (?, 64), (?, 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 80, 3000)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(-1, 128*5*3)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = MyNet().to(device)\n",
    "\n",
    "# 적절한 optimizer, Scheduler를 선택: torch.optim을 사용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=1e-4) # Adam Optimizer + L2 Regularization(weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler_gamma) # Exponential Learning Rate Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "# 적절한 Loss function을 선택: nn 모듈의 Function을 사용\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef65671-fa8c-496b-b1ba-410eccdbbdef",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b61782-28f3-492e-822c-8812cbfbdea9",
   "metadata": {},
   "source": [
    "## 3.1. Define Train, Evaluate, Predict Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dd7a94-a453-4481-a333-63b685bbaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_loader):\n",
    "    model.train() # 모델을 훈련 모드로 설정합니다.\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\") as pbar:\n",
    "        for features, labels in train_loader:\n",
    "            input_features = features['input_features']\n",
    "            input_features = input_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # optimizer의 gradient를 초기화합니다.\n",
    "            output = model(input_features) # 모델의 출력 결과를 저장합니다.\n",
    "            \n",
    "            loss = criterion(output, labels) # 손실함수를 계산합니다.\n",
    "            loss.backward() # 역전파를 실행합니다.\n",
    "            optimizer.step() # optimizer로 가중치를 업데이트합니다.\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "            pbar.update(1)\n",
    "      \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader), desc=\"Evaluating\") as pbar:\n",
    "            for features, labels in test_loader:\n",
    "                input_features = features['input_features']\n",
    "                input_features = input_features.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                output = model(input_features)\n",
    "                \n",
    "                test_loss += criterion(output, labels).item()\n",
    "                prediction = output.max(1, keepdim = True)[1]\n",
    "                correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "                pbar.update(1)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    scheduler.step(test_loss)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "def pred(model, test_loader):\n",
    "    model.eval()\n",
    "    pred_li = [] # 예측 결과를 모두 저장하는 리스트입니다.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader), desc=\"Predicting\") as pbar:\n",
    "            for features, labels in test_loader:\n",
    "                input_features = features['input_features']\n",
    "                input_features = input_features.to(device)\n",
    "                output = model(input_features)\n",
    "                pred_li.append(output.cpu().numpy().argmax(axis=1))\n",
    "                pbar.update(1)\n",
    "\n",
    "    return np.concatenate(pred_li) # 예측 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f5f81-0418-4140-a2aa-ade3bf25da1e",
   "metadata": {},
   "source": [
    "## 3.2. Define Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead454a8-8bba-4b37-a7cb-814dc6d79823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def main(model, config:Config, save_dir):\n",
    "    result_list = []\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    num_epochs = config.num_epochs\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss, train_accuracy = train(model, config.train_loader)\n",
    "        val_loss, val_accuracy = evaluate(model, config.test_loader)\n",
    "        print(f\"[EPOCH: {epoch}], \\tTrain Loss: {train_loss:.4f}, \\tTrain Accuracy: {train_accuracy:.2f} %, \\tVal Loss: {val_loss:.4f}, \\tVal Accuracy: {val_accuracy:.2f} % \\n\")\n",
    "        result = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }\n",
    "        scheduler.step(val_loss)\n",
    "        result_list.append(result)\n",
    "        \n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, os.path.join(save_dir, f'ckpt-{epoch}.pt'))\n",
    "            \n",
    "\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "    result_df.to_csv(f'{save_dir}/log_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7657712-5e9c-402d-96a1-3146da93522d",
   "metadata": {},
   "source": [
    "- 주의!! 아래는 실행에 주의할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f424ea2-7aa6-4cec-8fb0-8ed0341340fb",
   "metadata": {},
   "source": [
    "- 주의!! 아래는 실행에 주의할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86f1e3-2687-4856-ba82-0cbbe61d4b2b",
   "metadata": {},
   "source": [
    "- 주의!! 아래는 실행에 주의할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f613e7a-0ea8-4694-8edb-3d250255f510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [07:29<00:00,  2.29it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 1], \tTrain Loss: 1.4472, \tTrain Accuracy: 48.77 %, \tVal Loss: 0.8116, \tVal Accuracy: 71.77 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:06<00:00,  1.88it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 2], \tTrain Loss: 0.7715, \tTrain Accuracy: 72.24 %, \tVal Loss: 0.5108, \tVal Accuracy: 83.03 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:02<00:00,  1.90it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 3], \tTrain Loss: 0.5421, \tTrain Accuracy: 81.07 %, \tVal Loss: 0.4080, \tVal Accuracy: 85.89 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:47<00:00,  1.95it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 4], \tTrain Loss: 0.4115, \tTrain Accuracy: 85.95 %, \tVal Loss: 0.2852, \tVal Accuracy: 89.79 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:50<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 5], \tTrain Loss: 0.3368, \tTrain Accuracy: 88.64 %, \tVal Loss: 0.2689, \tVal Accuracy: 90.54 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:44<00:00,  1.96it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 6], \tTrain Loss: 0.2844, \tTrain Accuracy: 90.37 %, \tVal Loss: 0.2147, \tVal Accuracy: 92.34 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:34<00:00,  2.00it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 7], \tTrain Loss: 0.2442, \tTrain Accuracy: 91.72 %, \tVal Loss: 0.1742, \tVal Accuracy: 93.84 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:12<00:00,  1.86it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 8], \tTrain Loss: 0.2173, \tTrain Accuracy: 92.59 %, \tVal Loss: 0.1593, \tVal Accuracy: 94.29 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:12<00:00,  1.86it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 9], \tTrain Loss: 0.1937, \tTrain Accuracy: 93.45 %, \tVal Loss: 0.1480, \tVal Accuracy: 94.29 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:13<00:00,  1.86it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 10], \tTrain Loss: 0.1782, \tTrain Accuracy: 93.91 %, \tVal Loss: 0.1565, \tVal Accuracy: 94.14 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:08<00:00,  1.88it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:06<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 11], \tTrain Loss: 0.1176, \tTrain Accuracy: 95.98 %, \tVal Loss: 0.1125, \tVal Accuracy: 95.80 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:10<00:00,  1.87it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 12], \tTrain Loss: 0.1038, \tTrain Accuracy: 96.40 %, \tVal Loss: 0.1109, \tVal Accuracy: 96.10 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:04<00:00,  1.89it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 13], \tTrain Loss: 0.0943, \tTrain Accuracy: 96.71 %, \tVal Loss: 0.1055, \tVal Accuracy: 96.40 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:59<00:00,  1.91it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:05<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 14], \tTrain Loss: 0.0896, \tTrain Accuracy: 96.90 %, \tVal Loss: 0.1061, \tVal Accuracy: 95.80 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [09:01<00:00,  1.90it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 15], \tTrain Loss: 0.0818, \tTrain Accuracy: 97.17 %, \tVal Loss: 0.1012, \tVal Accuracy: 96.25 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:48<00:00,  1.95it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 16], \tTrain Loss: 0.0803, \tTrain Accuracy: 97.13 %, \tVal Loss: 0.1011, \tVal Accuracy: 96.10 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:58<00:00,  1.91it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 17], \tTrain Loss: 0.0798, \tTrain Accuracy: 97.20 %, \tVal Loss: 0.1017, \tVal Accuracy: 96.25 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:39<00:00,  1.98it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 18], \tTrain Loss: 0.0784, \tTrain Accuracy: 97.27 %, \tVal Loss: 0.1017, \tVal Accuracy: 96.25 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:46<00:00,  1.95it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 19], \tTrain Loss: 0.0771, \tTrain Accuracy: 97.36 %, \tVal Loss: 0.1018, \tVal Accuracy: 96.10 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████| 1029/1029 [08:50<00:00,  1.94it/s]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 20], \tTrain Loss: 0.0781, \tTrain Accuracy: 97.28 %, \tVal Loss: 0.1017, \tVal Accuracy: 96.10 % \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nclass Config:\\n    batch_size = 64\\n    learning_rate = 1e-3\\n    num_epochs = 20\\nscheuler: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patient=2) (적용 됨)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(model, config, './save06')\n",
    "\"\"\"\n",
    "class Config:\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 20\n",
    "scheuler: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patient=2) (적용 됨)\n",
    "\"\"\"\n",
    "# EPOCH 13 : Val Accuracy: 96.40%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd84a4-1383-4dc4-9fdc-14f83207354f",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c00f804-ec0d-4976-8495-4517933e5080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.tensor([[0.2, 0.3, 0.4], [0.3, 0.4, 0.2]]).max(1, keepdim = True)[1]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2f4339d-2364-4aa1-9c5c-020c305c1bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.tensor([1, 1])\n",
    "p.eq(l.view_as(p)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e73866-6f18-44fb-bb4f-06328c4d34eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
